{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4\n",
    "We will first attempt to implement the algorithm that is found in the paper. The\n",
    "DQL that we will be working on uses 2 methods (1) Experience replay and (2)\n",
    "Fixed target network to introduce efficient use of expeiences and stability in\n",
    "training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install gymnasium[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export MPS_ENABLE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=500)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ll be using experience replay memory for training our DQN. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "\n",
    "class SegmentTree(object):\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "        \"\"\"Build a Segment Tree data structure.\n",
    "        https://en.wikipedia.org/wiki/Segment_tree\n",
    "        Can be used as regular array, but with two\n",
    "        important differences:\n",
    "            a) setting item's value is slightly slower.\n",
    "               It is O(lg capacity) instead of O(1).\n",
    "            b) user has access to an efficient ( O(log segment size) )\n",
    "               `reduce` operation which reduces `operation` over\n",
    "               a contiguous subsequence of items in the array.\n",
    "        Paramters\n",
    "        ---------\n",
    "        capacity: int\n",
    "            Total size of the array - must be a power of two.\n",
    "        operation: lambda obj, obj -> obj\n",
    "            and operation for combining elements (eg. sum, max)\n",
    "            must form a mathematical group together with the set of\n",
    "            possible values for array elements (i.e. be associative)\n",
    "        neutral_element: obj\n",
    "            neutral element for the operation above. eg. float('-inf')\n",
    "            for max and 0 for sum.\n",
    "        \"\"\"\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
    "        self._capacity = capacity\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "        self._operation = operation\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self._operation(\n",
    "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "                )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        \"\"\"Returns result of applying `self.operation`\n",
    "        to a contiguous subsequence of the array.\n",
    "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
    "        Parameters\n",
    "        ----------\n",
    "        start: int\n",
    "            beginning of the subsequence\n",
    "        end: int\n",
    "            end of the subsequences\n",
    "        Returns\n",
    "        -------\n",
    "        reduced: obj\n",
    "            result of reducing self.operation over the specified range of array elements.\n",
    "        \"\"\"\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        # index of the leaf\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        \"\"\"Find the highest index `i` in the array such that\n",
    "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
    "        if array values are probabilities, this function\n",
    "        allows to sample indexes according to the discrete\n",
    "        probability efficiently.\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfixsum: float\n",
    "            upperbound on the sum of array prefix\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            highest index satisfying the prefixsum constraint\n",
    "        \"\"\"\n",
    "        try:\n",
    "            assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        except AssertionError:\n",
    "            print(\"Prefix sum error: {}\".format(prefixsum))\n",
    "            exit()\n",
    "        idx = 1\n",
    "        while idx < self._capacity:  # while non-leaf\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
    "\n",
    "        return super(MinSegmentTree, self).reduce(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','terminated'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class PrioritizedReplayMemory(object):\n",
    "    def __init__(self, size, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        super(PrioritizedReplayMemory, self).__init__()\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "        assert alpha >= 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame=1\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "\n",
    "    def push(self, *args):\n",
    "        data = Transition(*args)\n",
    "        idx = self._next_idx\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        return [self._storage[i] for i in idxes]\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        for _ in range(batch_size):\n",
    "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "\n",
    "        #find smallest sampling prob: p_min = smallest priority^alpha / sum of priorities^alpha\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame+=1\n",
    "        \n",
    "        #max_weight given to smallest prob\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float) \n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return encoded_sample\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = (priority+1e-5) ** self._alpha\n",
    "            self._it_min[idx] = (priority+1e-5) ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, (priority+1e-5))\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes = 64\n",
    "        self.layer1 = nn.Linear(n_observations, self.hidden_nodes)\n",
    "        self.layer2 = nn.Linear(self.hidden_nodes, self.hidden_nodes)\n",
    "        self.layer4 = nn.Linear(self.hidden_nodes, n_actions)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.layer1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.layer2.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.layer4.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward feed neural network call\n",
    "        \n",
    "        When called with x as a element, determine next action.\n",
    "        \n",
    "        When called with x as a batch, return tensor([[left0exp,right0exp]...]).\n",
    "\n",
    "        :param x: Input to learn\n",
    "        :type x: tensor\n",
    "        :return: \n",
    "        :rtype: tensor\n",
    "        \"\"\"\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer4(x)\n",
    "    \n",
    "class Duelnet(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(n_observations, 128) \n",
    "        self.A_head = torch.nn.Linear(128, n_actions)\n",
    "        self.V_head = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = self.A_head(F.relu(self.fc1(x)))\n",
    "        V = self.V_head(F.relu(self.fc1(x)))\n",
    "        Q = V + A - A.mean(1).view(-1, 1)  # Q(s,a)=V(s)+A(s,a)-mean(A(s,a))\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \"\"\"Base DQL class\"\"\"\n",
    "    BATCH_SIZE = 64 # BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "    GAMMA = 0.99 # GAMMA is the discount factor\n",
    "    LR = 0.0005 # LR is the learning rate of the ``Adam`` optimizer\n",
    "    REPLAY_CAPACITY = 2000 # REPLAY_CAPACITY is the maximum capacity of the replay buffer\n",
    "    MAX_RUN_WITH_GPU = 500 # MAX_RUN_WITH_GPU is the maximum number of runs with GPU\n",
    "    MAX_RUN_WITH_CPU = 100 # MAX_RUN_WITH_CPU is the maximum number of runs with CPU\n",
    "    C_STEP_RESET = 10 # C_STEP_RESET is the number of steps before update to target net\n",
    "    EPSILON_START = 1.0\n",
    "    EPSILON_DECAY = 2000\n",
    "    EPSILON_END = 0.05\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Get number of actions from gym action space\n",
    "        n_actions = env.action_space.n\n",
    "        # Get the number of state observations\n",
    "        state, info = env.reset()\n",
    "        n_observations = len(state)\n",
    "        self.episode_durations = []\n",
    "        self.epsilon = DQN.EPSILON_START\n",
    "        \n",
    "        \"\"\"Initialize replay memory D to capacity N\"\"\"\n",
    "        self.memory = ReplayMemory(DQN.REPLAY_CAPACITY) \n",
    "        \"\"\"Initialize action-value function Q with random weights $\\theta$\"\"\"\n",
    "        self.policy_net = Network(n_observations, n_actions)\n",
    "        \"\"\"Initialize target action-value function \\hat Q with weights $\\theta^-$\"\"\"\n",
    "        self.target_net = Network(n_observations, n_actions)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=DQN.LR)\n",
    "        self.loss_func = nn.SmoothL1Loss()\n",
    "    def select_action(self, state: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"With probability epsilon select a random action\n",
    "        otherwise select $a = \\arg\\max_a Q(s, a)$\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # take optmal action\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.policy_net(state)).view(1,1)\n",
    "        else:\n",
    "            # take a random action (exploration)\n",
    "            return torch.tensor([[env.action_space.sample()]],\n",
    "                                device=device, dtype=torch.int64)\n",
    "    def plot_durations(self,show_result=False):\n",
    "        plt.figure(1)\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        if show_result:\n",
    "            plt.title('Result')\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                display.display(plt.gcf())\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < DQN.BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(DQN.BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_next_states = torch.tensor([1 if s is False else 0 for s in batch.terminated]).unsqueeze(1)\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward).unsqueeze(1)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_state_batch).max(1).values\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = next_state_values * DQN.GAMMA * non_final_next_states + reward_batch\n",
    "    \n",
    "        loss = self.loss_func(state_action_values, expected_state_action_values)\n",
    "\n",
    "        \"\"\"Perform gradient descent with respect to network parameters\"\"\"\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    def start(self, env, num_episode_manual=0):\n",
    "        if torch.cuda.is_available():\n",
    "            num_episodes = DQN.MAX_RUN_WITH_GPU\n",
    "        else:\n",
    "            num_episodes = DQN.MAX_RUN_WITH_CPU\n",
    "        num_episodes = num_episode_manual if num_episode_manual > 0 else num_episodes\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            # Initialize the environment and get its state\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state,\n",
    "                                 dtype=torch.float32,\n",
    "                                 device=device).unsqueeze(0)\n",
    "            for t in count():\n",
    "                \"\"\"With probability select a random action a_t\"\"\"\n",
    "                action = self.select_action(state)\n",
    "                \"\"\"Execute action a_t in emulator and observe reward r_t\"\"\"\n",
    "                observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                done = terminated or truncated\n",
    "                next_state = torch.tensor(observation,\n",
    "                                        dtype=torch.float32,\n",
    "                                        device=device).unsqueeze(0)\n",
    "\n",
    "                \"\"\"Store transition (s_t, a_t, s_{t+1}, r_t) in D\"\"\"\n",
    "                self.memory.push(state, action, next_state, reward, terminated)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                self.optimize_model()\n",
    "                self.epsilon = (self.epsilon - self.EPSILON_END) * np.exp(- 1 / self.EPSILON_DECAY) + self.EPSILON_END\n",
    "\n",
    "                \"\"\"Every C steps reset \\hat Q = Q\"\"\"\n",
    "                if t % DQN.C_STEP_RESET == 0:\n",
    "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "                if done:\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    self.plot_durations()\n",
    "                    break\n",
    "        print('Complete')\n",
    "        self.plot_durations(show_result=True)\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "     \n",
    "class DoubleDQN(DQN):\n",
    "    \"\"\"Double DQL class\n",
    "    Tackles the problem of overestimation of Q-values.\n",
    "    \"\"\"\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < DQN.BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(DQN.BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_next_states = torch.tensor([1 if s is False else 0 for s in batch.terminated]).unsqueeze(1)\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward).unsqueeze(1)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "\n",
    "        # Get the policy net Q value\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch) # Q(s,a)\n",
    "        \n",
    "        # Get max action of target network\n",
    "        max_action = self.policy_net(next_state_batch).max(1,keepdim=True).indices\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_state_batch).gather(1, max_action)\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = next_state_values * DQN.GAMMA * non_final_next_states + reward_batch\n",
    "        \n",
    "        loss = self.loss_func(state_action_values, expected_state_action_values)\n",
    "\n",
    "        \"\"\"Perform gradient descent with respect to network parameters\"\"\"\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "class DuelingDQN(DQN):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Get number of actions from gym action space\n",
    "        n_actions = env.action_space.n\n",
    "        # Get the number of state observations\n",
    "        state, info = env.reset()\n",
    "        n_observations = len(state)\n",
    "        \n",
    "        \"\"\"Initialize action-value function Q with random weights $\\theta$\"\"\"\n",
    "        self.policy_net = Duelnet(n_observations, n_actions).to(device)\n",
    "        \"\"\"Initialize target action-value function \\hat Q with weights $\\theta^-$\"\"\"\n",
    "        self.target_net = Duelnet(n_observations, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "class DoubleDuelingDQN(DuelingDQN, DoubleDQN):\n",
    "    def optimize_model(self):\n",
    "        DoubleDQN.optimize_model(self)\n",
    "\n",
    "class PrioritizedDQN(DQN):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.memory = PrioritizedReplayMemory(DQN.REPLAY_CAPACITY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dqn = DQN()\n",
    "double_dqn = DoubleDQN()\n",
    "# dueling_dqn = DuelingDQN()\n",
    "# double_dueling_dqn = DoubleDuelingDQN()\n",
    "# prioritised_dqn = PrioritizedDQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=500)\n",
    "double_dqn.start(env,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = double_dqn.memory.sample(DQN.BATCH_SIZE)\n",
    "# Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "# detailed explanation). This converts batch-array of Transitions\n",
    "# to Transition of batch-arrays.\n",
    "batch = Transition(*zip(*transitions))\n",
    "\n",
    "# Compute a mask of non-final states and concatenate the batch elements\n",
    "# (a final state would've been the one after which simulation ended)\n",
    "non_final_next_states = torch.tensor([1 if s is False else 0 for s in batch.terminated]).unsqueeze(1)\n",
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.cat(batch.reward).unsqueeze(1)\n",
    "next_state_batch = torch.cat(batch.next_state)\n",
    "\n",
    "# Get the policy net Q value\n",
    "state_action_values = double_dqn.policy_net(state_batch).gather(1, action_batch) # Q(s,a)\n",
    "\n",
    "# Get max action of target network\n",
    "max_action = double_dqn.policy_net(next_state_batch).max(1).indices.view(-1, 1)\n",
    "with torch.no_grad():\n",
    "    next_state_values = double_dqn.target_net(next_state_batch).gather(1, max_action)\n",
    "# Compute the expected Q values\n",
    "expected_state_action_values = next_state_values * DQN.GAMMA * non_final_next_states + reward_batch\n",
    "\n",
    "loss = double_dqn.loss_func(state_action_values, expected_state_action_values)\n",
    "\n",
    "\"\"\"Perform gradient descent with respect to network parameters\"\"\"\n",
    "# Optimize the model\n",
    "double_dqn.optimizer.zero_grad()\n",
    "loss.backward()\n",
    "double_dqn.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state, _ = env.reset()\n",
    "state = torch.tensor(state,\n",
    "                        dtype=torch.float32,\n",
    "                        device=device).unsqueeze(0)\n",
    "a = double_dqn.select_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
